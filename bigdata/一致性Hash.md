> # `一致性Hash算法`

一致性Hash算法在1997年由麻省理工学院提出的一种分布式哈希（DHT）实现算法，设计目标是为了解决因特网中的热点（Hot Spot）问题，初衷和CARP十分相似。一致性Hash修正了CARP使用的简单哈希算法带来的问题，使得分布式哈希（DHT）可以在P2P环境中真正得到应用。

一致性Hash算法提出了在动态变化的Cache环境中，判定哈希算法好坏的四个定义：

1. **平衡性（Balance）**：平衡性是指哈希的结果能够尽可能分布在所有的缓冲(Cache)中去，这样可以使得所有的缓冲空间得到利用。很多哈希算法都能够满足这一条件。

2. **单调性（Monotonicity）**：单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应该能够保证原有已分配的内容可以被映射到原有的或者新的缓冲中去，而不会映射到旧的缓冲集合中的其他缓冲区。

3. **分散性（Spread）**：在分布式环境中，终端有可能看不到所有的缓冲，而只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上去，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应该能够尽量避免不一致的情况发生，也就是尽量降低分散性。

4. **负载（Load）**：负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射到不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。

> ### 只使用hash取模`hash(object)%N`算法

- 在分布式集群中，`对机器的添加删除，或者机器故障后自动脱落集群`这些操作是分布式集群管理最基本的功能。如果采用常用的hash取模`hash(object)%N`算法，那么在有机器添加或者删除后，`很多原有的数据就无法找到了，这样严重的违反了单调性原则`。
- 但对于分布式缓存这种的系统而言，映射规则失效就意味着`之前缓存的失效`，若同一时刻出现大量的缓存失效，则可能会出现 “缓存雪崩”，这将会造成灾难性的后果。


> ### 为对象选择服务器

将`对象Object`和`服务器Server`都放置到同一个`哈希环`后，在`哈希环`上顺时针查找距离这个对象的 `hash` 值最近的机器，即是这个对象所属的机器。 以 o2 对象为例，顺序针找到最近的机器是 cs2，故服务器 cs2 会缓存 o2 对象。而服务器 cs1 则缓存 o1，o3 对象，服务器 cs3 则缓存 o4 对象。

![image](https://segmentfault.com/img/bVbA7aH)

> ### 服务器增加的情况

假设由于业务需要，我们需要增加一台服务器 cs4，经过同样的 hash 运算，该服务器最终落于 t1 和 t2 服务器之间，具体如下图所示：

![image](https://segmentfault.com/img/bVbA7aI)

对于上述的情况，只有 t1 和 t2 服务器之间的对象需要重新分配。在以上示例中只有 `o3 对象需要重新分配`，即它被重新到 cs4 服务器。在前面我们已经分析过，如果使用简单的取模方法，当新添加服务器时可能会导致大部分缓存失效，而使用一致性哈希算法后，这种情况得到了较大的改善，因为`只有少部分对象需要重新分配`。

> ### 服务器减少的情况

假设 cs3 服务器出现故障导致服务下线，这时原本存储于 cs3 服务器的对象 o4，需要被重新分配至 cs2 服务器，其它对象仍存储在原有的机器上。

![image](https://segmentfault.com/img/bVbA7aJ)

> ### `虚拟节点`

根据上面的图解分析，一致性哈希算法满足了`单调性`和`负载均衡`的特性以及`一般hash算法的分散性`，但这还并不能当做其被广泛应用的原由，因为缺少了`平衡性`。下面将分析一致性哈希算法是如何满足平衡性的。`hash算法是不保证平衡性的`，如上面只部署了NODE1和NODE3的情况(NODE2被删除的图)，object1存储在NODE1中，而object2、object3、object4都存储在NODE3中，这样就造成了非常不平衡的状态。在一致性哈希算法中，为了尽可能的满足平衡性，其引入了虚拟节点。

**何为虚拟节点？**虚拟节点（Virtual Node）是实际节点（机器）在hash空间的复制品（replica），一个实际节点对应了若干个“虚拟节点”，这个对应个数也称为“复制个数”，“虚拟节点”在hash空间中以hash值排列。

这个算法的问题在于，`一个实际存储节点的加入或退出`，会影响`多个虚拟节点的重新分配`，进而影响很多节点参与到数据迁移中来；另外，实践中将一个虚拟节点重新分配给新的实际节点时需要将这部分数据遍历出来发送给新节点。我们需要一个跟合适的虚拟节点切分和分配方式，那就是`分片`。

> ### `分片`

`分片`将`哈希环`切割为`相同大小的分片`，然后将这些分片交给不同的节点负责。注意这里跟上面提到的虚拟节点有着很本质的区别，分片的`划分`和分片的`分配`被解耦，一个节点退出时，其所负责的分片并不需要顺时针合并给之后节点，而是可以更灵活的将`整个分片`作为一个整体交给任意节点，实践中，一个分片多作为最小的数据迁移和备份单位。

![image](https://res-static.hc-cdn.cn/fms/img/f2c984620f2913eb0e935eb8fc2740751603448371111)

> ### `Partition`

而也正是由于上面提到的`解耦`，相当于将原先的`key到节点`的映射拆成`两层`，需要一个新的机制来进行分片到存储节点的映射，由于分片数相对key空间已经很小并且数量确定，可以更精确地初始设置，并引入中心目录服务来根据`节点存活`修改`分片的映射关系`，同时将这个映射信息通知给所有的存储节点和客户端。下图是我们的分布式KV存储Zeppelin中的分片方式，Key Space通过Hash到分片，分片极其副本又通过一层映射到最终的存储节点Node Server。

![image](https://res-static.hc-cdn.cn/fms/img/142336224f9958f1f3bfa7940e8676ad1603448371111)